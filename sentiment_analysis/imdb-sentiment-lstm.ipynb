{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "imdb_sentiment_analysis_lstm.ipynb",
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "dsA-w5F15EZH"
      },
      "source": [
        "The dataset for this exercise is from \n",
        "\n",
        "https://github.com/udacity/deep-learning-v2-pytorch/tree/master/sentiment-rnn/data\n",
        "\n",
        "\n",
        "## Data Preprocessing"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "WeqGhIYm5N2q"
      },
      "source": [
        "with open('data/reviews.txt') as f:\n",
        "  reviews = f.read()\n",
        "with open('data/labels.txt') as f:\n",
        "  labels = f.read()"
      ],
      "execution_count": 1,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "EVb5WiBx541X",
        "outputId": "db784d73-5aa2-4e87-d95c-2d920b58f46c"
      },
      "source": [
        "print(reviews[:1000])"
      ],
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "bromwell high is a cartoon comedy . it ran at the same time as some other programs about school life  such as  teachers  . my   years in the teaching profession lead me to believe that bromwell high  s satire is much closer to reality than is  teachers  . the scramble to survive financially  the insightful students who can see right through their pathetic teachers  pomp  the pettiness of the whole situation  all remind me of the schools i knew and their students . when i saw the episode in which a student repeatedly tried to burn down the school  i immediately recalled . . . . . . . . . at . . . . . . . . . . high . a classic line inspector i  m here to sack one of your teachers . student welcome to bromwell high . i expect that many adults of my age think that bromwell high is far fetched . what a pity that it isn  t   \n",
            "story of a man who has unnatural feelings for a pig . starts out with a opening scene that is a terrific example of absurd comedy . a formal orchestra audience is turn\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "yqd8bhwd57TA",
        "outputId": "62dc63e9-a682-413a-b448-d570781c25e9"
      },
      "source": [
        "print(labels[:20])"
      ],
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "positive\n",
            "negative\n",
            "po\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "AmnwGk4hU0-I"
      },
      "source": [
        "### Remove Punctuation"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        },
        "id": "03S-jagt6jz0",
        "outputId": "2349b1ae-9de4-41f3-8b66-bbb8cf80c70a"
      },
      "source": [
        "from string import punctuation\n",
        "punctuation"
      ],
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            },
            "text/plain": [
              "'!\"#$%&\\'()*+,-./:;<=>?@[\\\\]^_`{|}~'"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 4
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "KOQ-0V3E6pOJ"
      },
      "source": [
        "reviews = reviews.lower()\n",
        "reviews = ''.join([char for char in reviews if char not in punctuation])"
      ],
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "m-DEi9Hz6z2J",
        "outputId": "66662e51-2208-42eb-b6d3-d46e774b9ac2"
      },
      "source": [
        "print(reviews[:1000])"
      ],
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "bromwell high is a cartoon comedy  it ran at the same time as some other programs about school life  such as  teachers   my   years in the teaching profession lead me to believe that bromwell high  s satire is much closer to reality than is  teachers   the scramble to survive financially  the insightful students who can see right through their pathetic teachers  pomp  the pettiness of the whole situation  all remind me of the schools i knew and their students  when i saw the episode in which a student repeatedly tried to burn down the school  i immediately recalled          at           high  a classic line inspector i  m here to sack one of your teachers  student welcome to bromwell high  i expect that many adults of my age think that bromwell high is far fetched  what a pity that it isn  t   \n",
            "story of a man who has unnatural feelings for a pig  starts out with a opening scene that is a terrific example of absurd comedy  a formal orchestra audience is turned into an insane  violent mo\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Nl60Njxx5byh",
        "outputId": "1385a13a-1bd4-425f-be1a-2c5e265d37aa"
      },
      "source": [
        "reviews = reviews.split('\\n')\n",
        "labels = labels.split('\\n')\n",
        "len(reviews), len(labels)"
      ],
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(25001, 25001)"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 7
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 154
        },
        "id": "DUtUyONi51cJ",
        "outputId": "c75b6edb-94e9-4382-d058-44296579af2f"
      },
      "source": [
        "reviews[0]"
      ],
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            },
            "text/plain": [
              "'bromwell high is a cartoon comedy  it ran at the same time as some other programs about school life  such as  teachers   my   years in the teaching profession lead me to believe that bromwell high  s satire is much closer to reality than is  teachers   the scramble to survive financially  the insightful students who can see right through their pathetic teachers  pomp  the pettiness of the whole situation  all remind me of the schools i knew and their students  when i saw the episode in which a student repeatedly tried to burn down the school  i immediately recalled          at           high  a classic line inspector i  m here to sack one of your teachers  student welcome to bromwell high  i expect that many adults of my age think that bromwell high is far fetched  what a pity that it isn  t   '"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 8
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "J58KdEIn6Q2n"
      },
      "source": [
        "words = ' '.join(reviews)\n",
        "words = words.split()"
      ],
      "execution_count": 9,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9EzkDpWwU714"
      },
      "source": [
        "### Word to Vector"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "SRjX-fqy5ihF"
      },
      "source": [
        "from collections import Counter"
      ],
      "execution_count": 10,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "3r-yswLG8V1e",
        "outputId": "65a4a600-1de5-4882-9a4c-baa525fd3765"
      },
      "source": [
        "word_counts = Counter(words)\n",
        "len(word_counts)"
      ],
      "execution_count": 11,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "74072"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 11
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "w1rFTLXV8kjM",
        "outputId": "3079e6b2-7ace-4e4a-b972-1a85b3dc6456"
      },
      "source": [
        "word_sorted_by_desc_cnt = sorted(word_counts, key=word_counts.get, reverse=True)\n",
        "print(word_sorted_by_desc_cnt[:10])"
      ],
      "execution_count": 12,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "['the', 'and', 'a', 'of', 'to', 'is', 'br', 'it', 'in', 'i']\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "HbZ7z6If9EvY"
      },
      "source": [
        "# indexing the word from 1, later we will use leading 0s to pad sequence \n",
        "# under a fixed length\n",
        "word2int = {word: idx+1 for idx, word in enumerate(word_sorted_by_desc_cnt)}"
      ],
      "execution_count": 13,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "I6_QNSl1_S4F",
        "outputId": "9adbeddd-c7c7-4891-f088-d216e00816ba"
      },
      "source": [
        "reviews_int = [\n",
        "  [word2int[word] for word in review.split()] for review in reviews]\n",
        "\n",
        "print(reviews_int[0])"
      ],
      "execution_count": 14,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[21025, 308, 6, 3, 1050, 207, 8, 2138, 32, 1, 171, 57, 15, 49, 81, 5785, 44, 382, 110, 140, 15, 5194, 60, 154, 9, 1, 4975, 5852, 475, 71, 5, 260, 12, 21025, 308, 13, 1978, 6, 74, 2395, 5, 613, 73, 6, 5194, 1, 24103, 5, 1983, 10166, 1, 5786, 1499, 36, 51, 66, 204, 145, 67, 1199, 5194, 19869, 1, 37442, 4, 1, 221, 883, 31, 2988, 71, 4, 1, 5787, 10, 686, 2, 67, 1499, 54, 10, 216, 1, 383, 9, 62, 3, 1406, 3686, 783, 5, 3483, 180, 1, 382, 10, 1212, 13583, 32, 308, 3, 349, 341, 2913, 10, 143, 127, 5, 7690, 30, 4, 129, 5194, 1406, 2326, 5, 21025, 308, 10, 528, 12, 109, 1448, 4, 60, 543, 102, 12, 21025, 308, 6, 227, 4146, 48, 3, 2211, 12, 8, 215, 23]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Y6FxQd-DVDDf"
      },
      "source": [
        "### Check for Outliers"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "SHZewB50_5Fa"
      },
      "source": [
        "import pandas as pd"
      ],
      "execution_count": 15,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "arIIYVSTIMSq",
        "outputId": "0d2ef880-09da-4931-e214-317199a65919"
      },
      "source": [
        "reviews_len = pd.DataFrame([len(review) for review in reviews_int], columns=['review_len'])\n",
        "reviews_len['review_len'].describe()"
      ],
      "execution_count": 16,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "count    25001.000000\n",
              "mean       240.798208\n",
              "std        179.020628\n",
              "min          0.000000\n",
              "25%        130.000000\n",
              "50%        179.000000\n",
              "75%        293.000000\n",
              "max       2514.000000\n",
              "Name: review_len, dtype: float64"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 16
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "X_-gWMICIZTX",
        "outputId": "deceabf9-3901-4f92-c82c-7b410414d9a7"
      },
      "source": [
        "reviews_len.groupby('review_len').size()[:10]"
      ],
      "execution_count": 17,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "review_len\n",
              "0     1\n",
              "10    1\n",
              "11    1\n",
              "12    2\n",
              "13    1\n",
              "14    1\n",
              "15    2\n",
              "16    2\n",
              "17    2\n",
              "19    2\n",
              "dtype: int64"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 17
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "3OTR9p7WQHb9"
      },
      "source": [
        "# remove the empty review and its corresponding label from dataset\n",
        "zero_indices = [idx for idx, review in enumerate(reviews_int) if len(review)==0]\n",
        "\n",
        "reviews_int = [review for idx, review in enumerate(reviews_int) if idx not in zero_indices]"
      ],
      "execution_count": 18,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "3ZRtZo2JJkG6"
      },
      "source": [
        "import numpy as np"
      ],
      "execution_count": 19,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "o0A8tGTHSAZr"
      },
      "source": [
        "labels = [label for idx, label in enumerate(labels) if idx not in zero_indices]"
      ],
      "execution_count": 20,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "198QThqlUCk9"
      },
      "source": [
        "### One-hot Encode Labels"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Txvfj0BNQuCm"
      },
      "source": [
        "encoded_labels = np.array([1 if label == 'positive' else 0 for label in labels])"
      ],
      "execution_count": 21,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "K9gX3eOtQSrr",
        "outputId": "9737487b-ed3b-4037-eb94-6d00a27e45c0"
      },
      "source": [
        "len(encoded_labels)"
      ],
      "execution_count": 22,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "25000"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 22
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "OSMtH8XX9gx5"
      },
      "source": [
        "### Check Unique Label Counts\n",
        "\n",
        "We want to check if our dataset is balanced before moving on to the next step"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "QQKEs-0b9lpe",
        "outputId": "00455806-bbe9-4601-abdf-493ab4fe1f5e"
      },
      "source": [
        "Counter(encoded_labels)"
      ],
      "execution_count": 23,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "Counter({0: 12500, 1: 12500})"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 23
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "VV44BmvQUHb9"
      },
      "source": [
        "### Pad Short Reviews and Truncate Long Reviews\n",
        "\n",
        "We will set the max length of the sequence to 200. Review under this length will be padded with leading zeros. This is the reason we start word2int at 1. We also need to remember that our vocab_size is (len(word2int) + 1)."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "dOeh96gTS8SI"
      },
      "source": [
        "def pad_sequence(reviews, sequence_len):\n",
        "  \n",
        "  num_rows = len(reviews)\n",
        "  padded_reviews = np.zeros((num_rows, sequence_len), dtype = int)\n",
        "  for idx, review in enumerate(reviews):\n",
        "    padded_reviews[idx, -len(review):] = review[: sequence_len]\n",
        "  \n",
        "  return padded_reviews"
      ],
      "execution_count": 24,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "j3M49TgyJQMM"
      },
      "source": [
        "# pad sequence shorter than 200 characters with leading 0s\n",
        "# cut sequence longer than 200 characters at 200\n",
        "\n",
        "sequence_len = 200\n",
        "\n",
        "reviews_array = pad_sequence(reviews_int, sequence_len=sequence_len)"
      ],
      "execution_count": 25,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1zah7FcQVbFE"
      },
      "source": [
        "### Train-Test-Validation Split\n",
        "\n",
        "We will use 80% of the data as training set, 10% of the data as validation set, and the rest as testing set."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Ozhkd5pkPpKw"
      },
      "source": [
        "train_test_split_ratio = 0.8"
      ],
      "execution_count": 26,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "p9POQ3PHWs1x"
      },
      "source": [
        "np.random.seed(42)"
      ],
      "execution_count": 27,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "an-UxN6ZXFKe",
        "outputId": "b40cfa79-ee3a-426b-c8c1-d0e80076ddf3"
      },
      "source": [
        "shuffled_idx = np.arange(len(reviews_array))\n",
        "\n",
        "np.random.shuffle(shuffled_idx)\n",
        "shuffled_idx"
      ],
      "execution_count": 28,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array([ 6868, 24016,  9668, ...,   860, 15795, 23654])"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 28
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "eqsSG-UwV0_w",
        "outputId": "0a08e07e-a058-4066-b073-2c01ad2d8e57"
      },
      "source": [
        "train_size = int(len(reviews_array) * train_test_split_ratio)\n",
        "train_indices = shuffled_idx[:train_size]\n",
        "\n",
        "remaining_indices = shuffled_idx[train_size: ]\n",
        "val_size = (len(reviews_array) - train_size) // 2\n",
        "val_indices =  remaining_indices[val_size:]\n",
        "test_indices =  remaining_indices[:val_size]\n",
        "\n",
        "train_x = reviews_array[train_indices, :]\n",
        "train_y = encoded_labels[train_indices]\n",
        "\n",
        "val_x = reviews_array[val_indices, :]\n",
        "val_y = encoded_labels[val_indices]\n",
        "\n",
        "test_x = reviews_array[test_indices, :]\n",
        "test_y = encoded_labels[test_indices]\n",
        "\n",
        "print(f\"{'train_x:': <8} {train_x.shape}, {'train_y:': <8} {train_y.shape}\")\n",
        "print(f\"{'val_x:': <8} {val_x.shape}, {'val_y:': <8} {val_y.shape}\")\n",
        "print(f\"{'test_x:': <8} {test_x.shape}, {'test_y:': <8} {test_y.shape}\")"
      ],
      "execution_count": 29,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "train_x: (20000, 200), train_y: (20000,)\n",
            "val_x:   (2500, 200), val_y:   (2500,)\n",
            "test_x:  (2500, 200), test_y:  (2500,)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ecB66KbRZOhQ"
      },
      "source": [
        "### Batching"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "kdOYsI4ka6lb"
      },
      "source": [
        "import torch\n",
        "from torch.utils.data import DataLoader, TensorDataset"
      ],
      "execution_count": 30,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "b9Nhhh4GbFjN"
      },
      "source": [
        "train_ds = TensorDataset(torch.from_numpy(train_x), torch.from_numpy(train_y))\n",
        "val_ds = TensorDataset(torch.from_numpy(val_x), torch.from_numpy(val_y))\n",
        "test_ds = TensorDataset(torch.from_numpy(test_x), torch.from_numpy(test_y))"
      ],
      "execution_count": 31,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "YtHGTrBGbeSp"
      },
      "source": [
        "batch_size = 50\n",
        "\n",
        "# If observations in dataloader cannot be divided by batch_size\n",
        "# we will need to drop the last incomplete batch as it will cause error \n",
        "# in LSTM layer later. \n",
        "# We also need to turn shuffle off for val_dl and test_dl, since we \n",
        "# want to concat the predictions and compare with the entire list of targets\n",
        "\n",
        "train_dl = DataLoader(train_ds, batch_size, shuffle=True, drop_last=True)\n",
        "val_dl = DataLoader(val_ds, batch_size, shuffle=False, drop_last=True)\n",
        "test_dl = DataLoader(test_ds, batch_size, shuffle=False, drop_last=True)"
      ],
      "execution_count": 32,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "NkgvNSzxbxuh",
        "outputId": "aaf1f098-4208-429b-c0a6-c0930f21e855"
      },
      "source": [
        "for batch_x, batch_y in train_dl:\n",
        "  print(batch_x.shape)\n",
        "  print(batch_y.shape)\n",
        "  break"
      ],
      "execution_count": 33,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "torch.Size([50, 200])\n",
            "torch.Size([50])\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Z24VDFAVcBLF"
      },
      "source": [
        "## Sentiment Network using LSTM"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "h1yVMV_DcwWR",
        "outputId": "3a299493-8836-4931-e349-b2934ccab9c2"
      },
      "source": [
        "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
        "\n",
        "print(f\"Device is {device}\")"
      ],
      "execution_count": 34,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Device is cuda\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "XTIqsxoNSixV"
      },
      "source": [
        "import torch.nn as nn"
      ],
      "execution_count": 35,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "oSqbuZNJqa08"
      },
      "source": [
        "class SentimentNet(nn.Module):\n",
        "  def __init__(self, vocab_size, embed_dim, hidden_size, num_lstm_layers, dropout_prob, out_size):\n",
        "    super().__init__()\n",
        "    self.vocab_size = vocab_size\n",
        "    self.embed_dim = embed_dim\n",
        "    self.hidden_size = hidden_size\n",
        "    self.num_lstm_layers = num_lstm_layers\n",
        "    self.dropout_prob = dropout_prob\n",
        "    self.out_size = out_size\n",
        "\n",
        "    self.embed = nn.Embedding(vocab_size, embed_dim)\n",
        "    self.lstm = nn.LSTM(\n",
        "        input_size=embed_dim, hidden_size=hidden_size, \n",
        "        num_layers=num_lstm_layers, batch_first=True, dropout=dropout_prob)\n",
        "    self.dropout = nn.Dropout(0.3)\n",
        "    self.fc = nn.Linear(in_features=hidden_size, out_features=out_size)\n",
        "    self.sigmoid = nn.Sigmoid()\n",
        "  \n",
        "  def forward(self, x, hidden):\n",
        "    batch_size = x.size(0)\n",
        "    out = self.embed(x)\n",
        "\n",
        "    out, hidden = self.lstm(out, hidden)\n",
        "\n",
        "    out = out.contiguous().view(-1, self.hidden_size)\n",
        "    out = self.dropout(out)\n",
        "    out = self.fc(out)\n",
        "\n",
        "    out = out.view(batch_size, -1)\n",
        "    out = self.sigmoid(out)\n",
        "    \n",
        "    # we only need the output at the end of the sequence\n",
        "    out = out[:, -1]\n",
        "\n",
        "    return out, hidden\n",
        "  \n",
        "  def init_hidden(self, batch_size):\n",
        "    # we use weight to keep the data type of the model parameters\n",
        "    weight = next(self.parameters()).data\n",
        "\n",
        "    hidden_state = (\n",
        "      weight.new(self.num_lstm_layers, batch_size, self.hidden_size).zero_().to(device),\n",
        "      weight.new(self.num_lstm_layers, batch_size, self.hidden_size).zero_().to(device))\n",
        "    \n",
        "    return hidden_state\n"
      ],
      "execution_count": 36,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "XBFm62hpw-4p"
      },
      "source": [
        "Before starting training, we want to make sure the dimensions in the model is correct. Let's instantiate a model and feed some data through it."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "jUw-4MwpqbRb",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "760dea90-ab30-4bed-900a-f58f85c46a22"
      },
      "source": [
        "# vacab_size should be word2int + 1 so that it includes the paddings\n",
        "vocab_size = len(word2int) + 1\n",
        "embed_dim = 3\n",
        "hidden_size = 4\n",
        "num_lstm_layers = 2\n",
        "dropout_prob = 0.3\n",
        "out_size = 1\n",
        "\n",
        "model = SentimentNet(\n",
        "    vocab_size, embed_dim, hidden_size, \n",
        "    num_lstm_layers, dropout_prob, out_size).to(device)\n",
        "\n",
        "model"
      ],
      "execution_count": 37,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "SentimentNet(\n",
              "  (embed): Embedding(74073, 3)\n",
              "  (lstm): LSTM(3, 4, num_layers=2, batch_first=True, dropout=0.3)\n",
              "  (dropout): Dropout(p=0.3, inplace=False)\n",
              "  (fc): Linear(in_features=4, out_features=1, bias=True)\n",
              "  (sigmoid): Sigmoid()\n",
              ")"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 37
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Qv1RW6pOwmc3",
        "outputId": "cf70d380-6b5d-4028-8161-dfcd4cae2149"
      },
      "source": [
        "hidden = model.init_hidden(batch_size)\n",
        "\n",
        "for batch_x, batch_y in train_dl:\n",
        "  out, hidden = model(batch_x.to(device), hidden)\n",
        "  break\n",
        "  \n",
        "print(out.shape, hidden[0].shape)"
      ],
      "execution_count": 38,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "torch.Size([50]) torch.Size([2, 50, 4])\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "NjAaeLFZcOE8"
      },
      "source": [
        "### Training"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "M6iSKIHU_K7j"
      },
      "source": [
        "PATH = 'model_checkpoint'"
      ],
      "execution_count": 39,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "5M2rm-WvxgDS"
      },
      "source": [
        "# vacab_size should be word2int + 1 so that it includes the paddings\n",
        "vocab_size = len(word2int) + 1\n",
        "embed_dim = 500\n",
        "hidden_size = 256\n",
        "num_lstm_layers = 2\n",
        "dropout_prob = 0.3\n",
        "out_size = 1\n",
        "\n",
        "model = SentimentNet(\n",
        "    vocab_size, embed_dim, hidden_size, \n",
        "    num_lstm_layers, dropout_prob, out_size).to(device)\n",
        "\n",
        "lr = 0.001\n",
        "\n",
        "criterion = nn.BCELoss()\n",
        "optimizer = torch.optim.Adam(model.parameters(), lr=lr)"
      ],
      "execution_count": 40,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "HwXBpqL1ydjN",
        "outputId": "04c51fec-62df-40a1-e344-61d778be7e65"
      },
      "source": [
        "epochs = 10\n",
        "\n",
        "grad_clip_thresh = 5\n",
        "log_every = 100\n",
        "\n",
        "counter = 0\n",
        "min_val_loss = np.infty\n",
        "\n",
        "for epoch in range(epochs):\n",
        "  \n",
        "  \n",
        "  hidden = model.init_hidden(batch_size)\n",
        "\n",
        "  for batch_x, batch_y in train_dl:\n",
        "    # train\n",
        "    model.train()\n",
        "    \n",
        "    counter += 1\n",
        "    batch_x, batch_y = batch_x.to(device), batch_y.to(device)\n",
        "\n",
        "    optimizer.zero_grad()\n",
        "    # detach hidden state before feeding it in to the model\n",
        "    hidden = tuple(state.data for state in hidden)\n",
        "    out, hidden = model(batch_x, hidden)\n",
        "\n",
        "    loss = criterion(out, batch_y.float())\n",
        "    loss.backward()\n",
        "\n",
        "    # clip gradient before updating the weight\n",
        "    nn.utils.clip_grad_norm_(model.parameters(), grad_clip_thresh)\n",
        "    optimizer.step()\n",
        "    # print(f\"\\r{counter}: {out.shape}\", end=\"\", flush=True)\n",
        "    print(f\"\\rEpoch: {epoch+1}, batch: {counter}, train_loss: {loss.item():.6f}\", end=\"\", flush=True)\n",
        "\n",
        "    \n",
        "    if counter % log_every == 0:\n",
        "      # validation mode\n",
        "      model.eval()\n",
        "\n",
        "      val_hidden = model.init_hidden(batch_size)\n",
        "      val_losses = []\n",
        "      for batch_x, batch_y in val_dl:\n",
        "        batch_x, batch_y = batch_x.to(device), batch_y.to(device)\n",
        "        val_hidden = tuple(state.data for state in val_hidden)\n",
        "        with torch.no_grad():\n",
        "          out, val_hidden = model(batch_x, val_hidden)\n",
        "          val_loss = criterion(out, batch_y.float())\n",
        "          val_losses.append(val_loss.item())\n",
        "      if np.mean(val_losses) < min_val_loss:\n",
        "        min_val_loss = val_loss\n",
        "        torch.save({\n",
        "            'epoch': epoch,\n",
        "            'model_state_dict': model.state_dict(),\n",
        "            'optimizer_state_dict': optimizer.state_dict(),\n",
        "            'train_loss': loss.item(),\n",
        "            'val_loss': val_loss.item(),\n",
        "            'model_hyperparameters': {\n",
        "                'vocab_size': model.vocab_size,\n",
        "                'embed_dim': model.embed_dim,\n",
        "                'hidden_size': model.hidden_size,\n",
        "                'num_lstm_layers': model.num_lstm_layers,\n",
        "                'dropout_prob': model.dropout_prob,\n",
        "                'out_size': model.out_size,\n",
        "            },\n",
        "        }, PATH)\n",
        "      print(f\"\\rEpoch: {epoch+1}, batch: {counter}, train_loss: {loss.item():.6f}, val_loss: {np.mean(val_losses):.6f}\")"
      ],
      "execution_count": 41,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Epoch: 1, batch: 100, train_loss: 0.663304, val_loss: 0.661472\n",
            "Epoch: 1, batch: 200, train_loss: 0.585354, val_loss: 0.580288\n",
            "Epoch: 1, batch: 300, train_loss: 0.635410, val_loss: 0.649840\n",
            "Epoch: 1, batch: 400, train_loss: 0.573247, val_loss: 0.515238\n",
            "Epoch: 2, batch: 500, train_loss: 0.454927, val_loss: 0.518009\n",
            "Epoch: 2, batch: 600, train_loss: 0.370448, val_loss: 0.430542\n",
            "Epoch: 2, batch: 700, train_loss: 0.301400, val_loss: 0.408504\n",
            "Epoch: 2, batch: 800, train_loss: 0.277392, val_loss: 0.392027\n",
            "Epoch: 3, batch: 900, train_loss: 0.243589, val_loss: 0.410152\n",
            "Epoch: 3, batch: 1000, train_loss: 0.240738, val_loss: 0.447372\n",
            "Epoch: 3, batch: 1100, train_loss: 0.296274, val_loss: 0.426252\n",
            "Epoch: 3, batch: 1200, train_loss: 0.340190, val_loss: 0.377071\n",
            "Epoch: 4, batch: 1300, train_loss: 0.091623, val_loss: 0.461042\n",
            "Epoch: 4, batch: 1400, train_loss: 0.195940, val_loss: 0.506260\n",
            "Epoch: 4, batch: 1500, train_loss: 0.399545, val_loss: 0.512712\n",
            "Epoch: 4, batch: 1600, train_loss: 0.151209, val_loss: 0.477477\n",
            "Epoch: 5, batch: 1700, train_loss: 0.323571, val_loss: 0.500945\n",
            "Epoch: 5, batch: 1800, train_loss: 0.178360, val_loss: 0.516984\n",
            "Epoch: 5, batch: 1900, train_loss: 0.062816, val_loss: 0.491185\n",
            "Epoch: 5, batch: 2000, train_loss: 0.072643, val_loss: 0.472913\n",
            "Epoch: 6, batch: 2100, train_loss: 0.012732, val_loss: 0.553276\n",
            "Epoch: 6, batch: 2200, train_loss: 0.101215, val_loss: 0.575898\n",
            "Epoch: 6, batch: 2300, train_loss: 0.063452, val_loss: 0.690047\n",
            "Epoch: 6, batch: 2400, train_loss: 0.046125, val_loss: 0.560946\n",
            "Epoch: 7, batch: 2500, train_loss: 0.063675, val_loss: 0.565958\n",
            "Epoch: 7, batch: 2600, train_loss: 0.017088, val_loss: 0.658620\n",
            "Epoch: 7, batch: 2700, train_loss: 0.148223, val_loss: 0.702965\n",
            "Epoch: 7, batch: 2800, train_loss: 0.027135, val_loss: 0.651108\n",
            "Epoch: 8, batch: 2900, train_loss: 0.011811, val_loss: 0.715335\n",
            "Epoch: 8, batch: 3000, train_loss: 0.011056, val_loss: 0.724398\n",
            "Epoch: 8, batch: 3100, train_loss: 0.160627, val_loss: 0.772722\n",
            "Epoch: 8, batch: 3200, train_loss: 0.029730, val_loss: 0.752353\n",
            "Epoch: 9, batch: 3300, train_loss: 0.092206, val_loss: 0.804018\n",
            "Epoch: 9, batch: 3400, train_loss: 0.012996, val_loss: 0.817511\n",
            "Epoch: 9, batch: 3500, train_loss: 0.041766, val_loss: 0.791893\n",
            "Epoch: 9, batch: 3600, train_loss: 0.018836, val_loss: 0.833618\n",
            "Epoch: 10, batch: 3700, train_loss: 0.001374, val_loss: 0.822136\n",
            "Epoch: 10, batch: 3800, train_loss: 0.034889, val_loss: 0.902123\n",
            "Epoch: 10, batch: 3900, train_loss: 0.001801, val_loss: 0.817280\n",
            "Epoch: 10, batch: 4000, train_loss: 0.009253, val_loss: 0.854419\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1kM1iZr2cSo-"
      },
      "source": [
        "### Testing"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Go5XG3e8f5zH",
        "outputId": "4bbacec4-ca9b-4596-fecd-8ff698660f9d"
      },
      "source": [
        "checkpoint = torch.load(PATH)\n",
        "print(f\"checkpoint val_loss: {val_loss}\")\n",
        "model = SentimentNet(**checkpoint['model_hyperparameters'])\n",
        "model.load_state_dict(checkpoint['model_state_dict'])\n",
        "model.to(device)\n",
        "# optimizer.load_state_dict(checkpoint['optimizer_state_dict'])\n",
        "model"
      ],
      "execution_count": 42,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "checkpoint val_loss: 0.9944574236869812\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "SentimentNet(\n",
              "  (embed): Embedding(74073, 500)\n",
              "  (lstm): LSTM(500, 256, num_layers=2, batch_first=True, dropout=0.3)\n",
              "  (dropout): Dropout(p=0.3, inplace=False)\n",
              "  (fc): Linear(in_features=256, out_features=1, bias=True)\n",
              "  (sigmoid): Sigmoid()\n",
              ")"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 42
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "sF_0Doq-lKpJ"
      },
      "source": [
        "from sklearn.metrics import confusion_matrix, accuracy_score"
      ],
      "execution_count": 43,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "gQWs1sSjgC5b",
        "outputId": "daa94c86-d2bd-4ce3-b885-d071eae4310f"
      },
      "source": [
        "# validation mode\n",
        "model.eval()\n",
        "\n",
        "test_hidden = model.init_hidden(batch_size)\n",
        "test_losses = []\n",
        "test_preds = torch.tensor([])\n",
        "for batch_x, batch_y in test_dl:\n",
        "  batch_x, batch_y = batch_x.to(device), batch_y.to(device)\n",
        "  test_hidden = tuple(state.data for state in test_hidden)\n",
        "  \n",
        "  with torch.no_grad():\n",
        "    out, test_hidden = model(batch_x, test_hidden)\n",
        "    test_loss = criterion(out, batch_y.float())\n",
        "    test_losses.append(test_loss.item())\n",
        "    batch_pred = torch.round(out).cpu()\n",
        "    test_preds = torch.cat((test_preds, batch_pred), dim=0)\n",
        "\n",
        "   \n",
        "print(f\"test_loss: {np.mean(test_losses):.6f}\")\n",
        "\n",
        "cm = confusion_matrix(\n",
        "    test_ds.tensors[1].to('cpu').numpy(), test_preds.to('cpu').numpy())\n",
        "# TN, FN, TP, FP = cm[0, 0], cm[1, 0], cm[1, 1], cm[0, 1]\n",
        "accuracy = accuracy_score(\n",
        "    test_ds.tensors[1].to('cpu').numpy(), test_preds.to('cpu').numpy())\n",
        "print(f\"test_accuracy: {accuracy:.6f}\")\n",
        "print(f\"confusion matrix:\\n{cm}\")"
      ],
      "execution_count": 44,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "test_loss: 0.371326\n",
            "test_accuracy: 0.844800\n",
            "confusion matrix:\n",
            "[[1047  200]\n",
            " [ 188 1065]]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5mwqeV2JcWBl"
      },
      "source": [
        "### Prediction"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "mP-H-wA48ULf"
      },
      "source": [
        "Let's use our model to predict the sentiment of [this positive review](https://readysteadycut.com/2020/05/28/dorohedoro-season-1-netflix-review/) of [Netflix's Dorohedoro](https://www.netflix.com/title/80991903#:~:text=2020%20%7C%20TV%2DMA%20%7C%201,Takagi%2C%20Reina%20Kondo%2C%20Kenyu%20Horiuchi)"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "-zGDX_Gfxe6u"
      },
      "source": [
        "# https://readysteadycut.com/2020/05/28/dorohedoro-season-1-netflix-review/\n",
        "\n",
        "new_review = \"\"\"Netflix has a winner on its hands with new original anime Dorohedoro, a 12-part medley of violence, eccentricity, and class-conscious world-building that bows out unflatteringly begging for a sequel but does a fine job of making a case for one throughout its run.\n",
        "\n",
        "\n",
        "Snappy in terms of both its dialogue and its dinosaur-headed hero, Caiman (Wataru Takagi), an amnesiac with a particular grudge against the oppressive Sorcerers who meddle with the lives and biology of the citizens in the decrepit “Hole”, Dorohedoro does an admirable job of fleshing out its world and cast while maintaining a steady drip-feed of dramatic secrets and blood-soaked violence.\n",
        "\n",
        "\n",
        "There’s plenty to like here in terms of characterization, world-building, and plotting, even if some of the mysteries are necessarily left unresolved. The CG animation, too, works well in its bloody broad strokes, and while narratively and aesthetically Dorohedoro lacks a degree of subtlety, you can’t help but imagine that’s very much the point.\n",
        "\n",
        "Perhaps, though, that subtlety is to be found in the characterization, which treats figures on both sides of the class divide to rounded personalities or at least entertainingly eccentric traits. While Caiman is inarguably the lead, the focus is divided evenly enough that your favorite character could conceivably be anyone.\n",
        "\n",
        "Dorohedoro’s long-term success on Netflix will depend, one assumes, on how people take to its wide-open ending, which might not work as a satisfying conclusion. Whether that puts people off or entices them to the follow-up remains to be seen, but all the requisite elements are here for a well-liked original anime that’ll attract an enthusiastic fan base over the coming weeks.\n",
        "\"\"\""
      ],
      "execution_count": 45,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "L8CnOwnQs0Su"
      },
      "source": [
        "def tokenize_review(text_review):\n",
        "  # review to lower case, remove punctuation, and to list of words\n",
        "  review_cleaned = ''.join([char for char in text_review.lower() if char not in punctuation]).split()\n",
        "  # map word to integer\n",
        "  review_int = [word2int[word] for word in review_cleaned if word in word2int]\n",
        "  return review_int"
      ],
      "execution_count": 46,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "NhuhtJcoq8FX"
      },
      "source": [
        "new_review_int = tokenize_review(new_review)"
      ],
      "execution_count": 47,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "68wJCikmsOxN"
      },
      "source": [
        "new_reviews_array = pad_sequence([new_review_int], sequence_len)"
      ],
      "execution_count": 48,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "QZB1ej_cq4jO"
      },
      "source": [
        "new_reviews_tensors = torch.from_numpy(new_reviews_array)"
      ],
      "execution_count": 49,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "u2NLJuxGtkKH",
        "outputId": "6a8a8be4-522c-4b50-f520-6f13f203de3e"
      },
      "source": [
        "new_reviews_tensors.shape"
      ],
      "execution_count": 50,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "torch.Size([1, 200])"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 50
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "gsVsh9W9sEDS",
        "outputId": "ef94ce6c-8fe4-4bda-bdf4-7fd7e7a3deda"
      },
      "source": [
        "model.eval()\n",
        "with torch.no_grad():\n",
        "  hidden = model.init_hidden(1)\n",
        "  out, hidden = model(new_reviews_tensors.to(device), hidden)\n",
        "print(f\"out: {out.cpu().item():.6f}\")\n",
        "print(f\"pred: {torch.round(out).cpu().item():.0f}\")"
      ],
      "execution_count": 51,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "out: 0.967452\n",
            "pred: 1\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "YIMOMnOHzUKs"
      },
      "source": [
        ""
      ],
      "execution_count": 51,
      "outputs": []
    }
  ]
}